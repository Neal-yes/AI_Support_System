name: Release Backup Artifacts

on:
  workflow_dispatch:
    inputs:
      run_id:
        description: "Optional: backup-restore run_id to release (defaults to latest successful)"
        required: false
        type: string
      tag:
        description: "Optional: tag name for release (defaults to auto-generated)"
        required: false
        type: string
      name:
        description: "Optional: release name (defaults to auto-generated)"
        required: false
        type: string
      force:
        description: "Force update release body even if content hash unchanged"
        required: false
        type: boolean
  workflow_run:
    workflows: ["Backup & Restore Qdrant"]
    types:
      - completed
    branches:
      - main

permissions:
  contents: write
  actions: read

jobs:
  release-backup:
    runs-on: ubuntu-latest
    timeout-minutes: 20
    # Run when: manual, or backup-restore completed successfully on main
    if: >-
      ${{ github.event_name == 'workflow_dispatch' ||
          (github.event_name == 'workflow_run' &&
           github.event.workflow_run.conclusion == 'success' &&
           github.event.workflow_run.head_branch == 'main') }}
    env:
      # When triggered by workflow_run, provide the completed backup-restore run id
      RUN_ID: ${{ github.event.workflow_run.id }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Resolve target backup-restore run
        id: resolve_run
        uses: actions/github-script@v7
        with:
          script: |
            const envRun = process.env.RUN_ID || '';
            const runInput = core.getInput('run_id');

            // Helper: find workflow by file path
            async function getWorkflowByPath(path) {
              const { data } = await github.rest.actions.listRepoWorkflows({
                owner: context.repo.owner,
                repo: context.repo.repo,
                per_page: 100,
              });
              const wf = data.workflows.find(w => w.path === path);
              if (!wf) throw new Error(`Workflow not found by path: ${path}`);
              return wf;
            }

            let runId = 0;
            if (envRun && String(envRun).trim().length) {
              runId = parseInt(String(envRun).trim(), 10);
            } else if (runInput && runInput.trim().length) {
              runId = parseInt(runInput.trim(), 10);
            }
            if (!runId) {
              const wf = await getWorkflowByPath('.github/workflows/backup-restore.yml');
              const { data } = await github.rest.actions.listWorkflowRuns({
                owner: context.repo.owner,
                repo: context.repo.repo,
                workflow_id: wf.id,
                status: 'success',
                per_page: 1,
              });
              if (!data.workflow_runs || !data.workflow_runs.length) {
                throw new Error('No successful backup-restore runs found');
              }
              runId = data.workflow_runs[0].id;
            }

            core.setOutput('run_id', String(runId));

      - name: List artifacts for target run
        id: list_artifacts
        uses: actions/github-script@v7
        env:
          TARGET_RUN_ID: ${{ steps.resolve_run.outputs.run_id }}
        with:
          script: |
            const runIdSrc = process.env.TARGET_RUN_ID || core.getInput('run_id');
            const run_id = parseInt(runIdSrc, 10);
            const { data } = await github.rest.actions.listWorkflowRunArtifacts({
              owner: context.repo.owner,
              repo: context.repo.repo,
              run_id,
              per_page: 100,
            });
            if (!data.artifacts || !data.artifacts.length) {
              throw new Error(`No artifacts found for run ${run_id}`);
            }
            // Prefer artifact starting with 'backup-restore-' otherwise take the first
            let artifact = data.artifacts.find(a => a.name.startsWith('backup-restore-')) || data.artifacts[0];
            core.setOutput('artifact_id', String(artifact.id));
            core.setOutput('artifact_name', artifact.name);

      - name: Download artifact zip via API
        env:
          GH_TOKEN: ${{ github.token }}
        run: |
          # be tolerant to missing optional inputs/artifacts
          set +e
          set -o pipefail
          run_id="${{ steps.resolve_run.outputs.run_id }}"
          artifact_id="${{ steps.list_artifacts.outputs.artifact_id }}"
          artifact_name="${{ steps.list_artifacts.outputs.artifact_name }}"
          out_dir="release_artifacts/${run_id}"
          mkdir -p "$out_dir"
          echo "Downloading artifact id=$artifact_id name=$artifact_name for run=$run_id"
          curl -fsSL -H "Authorization: Bearer $GH_TOKEN" \
            -H "Accept: application/vnd.github+json" \
            -o "$out_dir/${artifact_name}.zip" \
            "https://api.github.com/repos/${{ github.repository }}/actions/artifacts/${artifact_id}/zip"
          echo "Computing sha256"
          (cd "$out_dir" && shasum -a 256 "${artifact_name}.zip" | tee "${artifact_name}.zip.sha256")
          echo "out_dir=$out_dir" >> "$GITHUB_OUTPUT"

      - name: Extract artifact and generate validation summary
        run: |
          # be tolerant to missing optional inputs/artifacts
          set -uo pipefail
          run_id="${{ steps.resolve_run.outputs.run_id }}"
          artifact_name="${{ steps.list_artifacts.outputs.artifact_name }}"
          out_dir="release_artifacts/${run_id}"
          extract_dir="$out_dir/extract"
          mkdir -p "$extract_dir"
          sudo apt-get update -y
          sudo apt-get install -y jq unzip
          # Ensure token available for GitHub API requests
          export GH_TOKEN="${GH_TOKEN:-${GITHUB_TOKEN:-}}"
          echo "Listing zip entries"
          entries=$(unzip -Z1 "$out_dir/${artifact_name}.zip")
          echo "$entries" | sed -n '1,100p'
          echo "Extracting entire archive to $extract_dir for robust file discovery ..."
          unzip -o "$out_dir/${artifact_name}.zip" -d "$extract_dir" >/dev/null || true
          emb_path=$(find "$extract_dir" -type f -name 'embedding_upsert.json' | head -n1 || true)
          dump_path=$(find "$extract_dir" -type f -regex '.*/qdrant_.*_dump.json' | head -n1 || true)
          ci_found=$(find "$extract_dir" -type f -name 'ci_summary.txt' | head -n1 || true)
          e2e_found=$(find "$extract_dir" -type f -name 'metrics_e2e_summary.txt' | head -n1 || true)
          if [ -n "$emb_path" ]; then cp -f "$emb_path" "$out_dir/embedding_upsert.json" || true; fi
          if [ -n "$dump_path" ]; then cp -f "$dump_path" "$out_dir/qdrant_dump.json" || true; fi
          if [ -n "$ci_found" ]; then cp -f "$ci_found" "$out_dir/ci_summary.txt" || true; fi
          if [ -n "$e2e_found" ]; then cp -f "$e2e_found" "$out_dir/metrics_e2e_summary.txt" || true; fi
          # If metrics_e2e_summary.txt or api_metrics.prom still missing, fetch from latest successful Metrics E2E run
          # Also try to fetch api_metrics.prom for fallback metric computation
          if [ ! -s "$out_dir/metrics_e2e_summary.txt" ] || [ ! -s "$out_dir/api_metrics.prom" ]; then
            echo "metrics_e2e_summary.txt not found in artifact, fetching from latest Metrics E2E run..."
            wf_json="$out_dir/_workflows.json"
            runs_json="$out_dir/_metrics_e2e_runs.json"
            arts_json="$out_dir/_metrics_e2e_artifacts.json"
            curl -fsSL -H "Authorization: Bearer $GH_TOKEN" -H "Accept: application/vnd.github+json" \
              "https://api.github.com/repos/${{ github.repository }}/actions/workflows" -o "$wf_json" || true
            wf_id=$(jq -r '.workflows[] | select(.name=="Metrics E2E") | .id' "$wf_json" | head -n1 || true)
            if [ -n "$wf_id" ]; then
              # Restrict to the same branch to avoid picking an older successful run from another branch without the desired artifacts
              curl -fsSL -H "Authorization: Bearer $GH_TOKEN" -H "Accept: application/vnd.github+json" \
                "https://api.github.com/repos/${{ github.repository }}/actions/workflows/${wf_id}/runs?status=success&branch=${GITHUB_REF_NAME}&per_page=1" -o "$runs_json" || true
              m_run=$(jq -r '.workflow_runs[0].id // empty' "$runs_json" || true)
              if [ -n "$m_run" ]; then
                curl -fsSL -H "Authorization: Bearer $GH_TOKEN" -H "Accept: application/vnd.github+json" \
                  "https://api.github.com/repos/${{ github.repository }}/actions/runs/${m_run}/artifacts" -o "$arts_json" || true
                # Prefer the metrics artifacts uploaded by Metrics E2E job
                art_id=$(jq -r '.artifacts[] | select(.name|test("^metrics-artifacts")) | .id' "$arts_json" | head -n1 || true)
                if [ -z "$art_id" ]; then
                  art_id=$(jq -r '.artifacts[0].id // empty' "$arts_json" || true)
                fi
                if [ -n "$art_id" ]; then
                  zip_path="$out_dir/_metrics_e2e_${m_run}.zip"
                  tmp_ext="$out_dir/_metrics_e2e_extract_${m_run}"
                  mkdir -p "$tmp_ext"
                  curl -fsSL -H "Authorization: Bearer $GH_TOKEN" -H "Accept: application/vnd.github+json" \
                    -o "$zip_path" \
                    "https://api.github.com/repos/${{ github.repository }}/actions/artifacts/${art_id}/zip" || true
                  unzip -o "$zip_path" -d "$tmp_ext" >/dev/null || true
                  found=$(find "$tmp_ext" -type f -name 'metrics_e2e_summary.txt' | head -n1 || true)
                  if [ -n "$found" ]; then
                    cp -f "$found" "$out_dir/metrics_e2e_summary.txt" || true
                  fi
                  found_api=$(find "$tmp_ext" -type f -name 'api_metrics.prom' | head -n1 || true)
                  if [ -n "$found_api" ]; then
                    cp -f "$found_api" "$out_dir/api_metrics.prom" || true
                  fi
                fi
              fi
            fi
          fi
          # Fallback to filesystem search only if needed
          if [ -z "$emb_path" ] || [ -z "$dump_path" ]; then
            echo "Falling back to filesystem search in extract dir..."
            unzip -o "$out_dir/${artifact_name}.zip" -d "$extract_dir" >/dev/null || true
            echo "Extraction tree (top 200 lines):"
            (cd "$extract_dir" && find . -maxdepth 4 -type f -printf '%p\n' | sed -n '1,200p') || true
            [ -z "$emb_path" ] && emb_path=$(find "$extract_dir" -type f -name 'embedding_upsert.json' | head -n1)
            [ -z "$dump_path" ] && dump_path=$(find "$extract_dir" -type f -regex '.*/qdrant_.*_dump.json' | head -n1)
          fi
          if [ -z "$emb_path" ] || [ -z "$dump_path" ]; then
            echo "[WARN] mandatory artifact files missing (embedding_upsert.json or qdrant_*_dump.json)" >&2
            echo "Extraction tree (top 200 lines):"
            (cd "$extract_dir" && find . -maxdepth 4 -type f -printf '%p\n' | sed -n '1,200p') || true
          fi
          echo "emb_path=$emb_path"
          echo "dump_path=$dump_path"
          exp_total=$(jq -r '.total' "$emb_path")
          exp_src=$(jq -r '.src' "$emb_path")
          exp_coll=$(jq -r '.collection' "$emb_path")
          echo "Derived expects: total=$exp_total src=$exp_src coll=$exp_coll"
          # Run validation script to generate a friendly summary; never fail release even if validation fails
          if [ -n "$emb_path" ] && [ -n "$dump_path" ]; then
            python3 scripts/validate_backup_artifacts.py \
              --emb "$emb_path" \
              --dump "$dump_path" \
              --expect-total "$exp_total" \
              --expect-src "$exp_src" \
              --expect-collection "$exp_coll" \
              || true
          else
            echo "[WARN] Skipping validation due to missing artifact files"
          fi
          # Run validation script to generate a friendly summary; never fail release even if validation fails
          python3 scripts/validate_backup_artifacts.py \
            --emb "$emb_path" \
            --dump "$dump_path" \
            --expect-total "$exp_total" \
            --expect-src "$exp_src" \
            --expect-collection "$exp_coll" \
            --sample-count 3 \
            > "$out_dir/VALIDATION_SUMMARY.txt" || true

      - name: Generate release notes
        id: notes
        env:
          GH_TOKEN: ${{ github.token }}
          TAG_NAME: ${{ inputs.tag || format('backup-restore-run-{0}', steps.resolve_run.outputs.run_id) }}
        run: |
          # Be tolerant to missing optional inputs/artifacts. Never fail this step.
          set +e
          set +o pipefail || true
          trap '' ERR
          set -o pipefail
          run_id="${{ steps.resolve_run.outputs.run_id }}"
          artifact_name="${{ steps.list_artifacts.outputs.artifact_name }}"
          out_dir="release_artifacts/${run_id}"
          ts=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
          mkdir -p "$out_dir" || true
          : > "$out_dir/RELEASE_NOTES.md"
          cat > "$out_dir/RELEASE_NOTES.md" << 'EOF'
          # Backup & Restore Artifacts

          本次 Release 来源于最近一次成功的 Backup & Restore 工作流运行，包含以下内容：
          
          - 工件：ZIP 压缩包（内含 embedding_upsert.json 与 qdrant dump）
          - 完整性校验：提供 sha256 摘要文件（.sha256）
          - 运行来源：GitHub Actions backup-restore 工作流

          使用方法：
          1) 下载 ZIP 与其对应的 .sha256 文件
          2) 本地校验：`shasum -a 256 -c <artifact>.zip.sha256`
          3) 解压并按 Playbook 进行校验或恢复演练

          参考文档：docs/backup_restore_playbook.md
          EOF
          # Derive Run Metrics
          metrics_section=""
          emb_json="$out_dir/embedding_upsert.json"
          dump_json="$out_dir/qdrant_dump.json"
          coll=""
          src=""
          total=""
          restored_total=""
          if [ -f "$emb_json" ]; then
            coll=$(jq -r '.collection // empty' "$emb_json" || true)
            src=$(jq -r '.src // empty' "$emb_json" || true)
            total=$(jq -r '.total // empty' "$emb_json" || true)
          fi
          if [ -f "$dump_json" ]; then
            restored_total=$(jq -r 'if type=="object" then (.points // []) | length else length end' "$dump_json" || true)
          fi
          # RTO seconds from workflow run timing
          rto_secs=""
          run_json="$out_dir/run_${run_id}.json"
          curl -fsSL -H "Authorization: Bearer $GH_TOKEN" -H "Accept: application/vnd.github+json" \
            "https://api.github.com/repos/${{ github.repository }}/actions/runs/${run_id}" -o "$run_json" || true
          if [ -s "$run_json" ]; then
            started=$(jq -r '.run_started_at // .created_at // empty' "$run_json" || true)
            updated=$(jq -r '.updated_at // .completed_at // empty' "$run_json" || true)
            if [ -n "$started" ] && [ -n "$updated" ]; then
              start_epoch=$(date -d "$started" +%s || true)
              end_epoch=$(date -d "$updated" +%s || true)
              if [ -n "$start_epoch" ] && [ -n "$end_epoch" ]; then
                rto_secs=$(( end_epoch - start_epoch ))
              fi
            fi
          fi
          {
            echo;
            echo "## Run Metrics";
            echo;
            [ -n "$coll" ] && echo "- collection: $coll" || true;
            [ -n "$src" ] && echo "- src: $src" || true;
            [ -n "$total" ] && echo "- seed_total: $total" || true;
            [ -n "$restored_total" ] && echo "- restored_total: $restored_total" || true;
            [ -n "$rto_secs" ] && echo "- RTO_seconds: $rto_secs" || true;
          } >> "$out_dir/RELEASE_NOTES.md"
          if [ -f "$out_dir/VALIDATION_SUMMARY.txt" ]; then
            {
              echo;
              echo "## Artifact Validation (auto-generated)";
              echo;
              echo '```text';
              cat "$out_dir/VALIDATION_SUMMARY.txt";
              echo '```';
            } >> "$out_dir/RELEASE_NOTES.md"
          fi
          if [ -f "$out_dir/ci_summary.txt" ]; then
            {
              echo;
              echo "## CI Summary";
              echo;
              echo '```text';
              cat "$out_dir/ci_summary.txt";
              echo '```';
            } >> "$out_dir/RELEASE_NOTES.md"
          fi
          # Append Key Signals section (quick glance) - always attempt; internal logic provides N/A fallback
          if :; then
            ks_file="$out_dir/_key_signals.txt"
            cs_main="$out_dir/ci_summary.txt"
            cs_alt="$out_dir/metrics_e2e_summary.txt"
            # Proactively fetch Metrics E2E artifact here if neither summary nor prom exists in out_dir
            if [ ! -s "$cs_alt" ] && [ ! -s "$out_dir/api_metrics.prom" ]; then
              echo "[notes] Fetching latest successful Metrics E2E artifact into out_dir ..." >&2
              GH_TOKEN_EFF="${GH_TOKEN:-${GITHUB_TOKEN:-}}"
              wf_json_n="$out_dir/_workflows_for_notes.json"
              runs_json_n="$out_dir/_metrics_e2e_runs_for_notes.json"
              arts_json_n="$out_dir/_metrics_e2e_artifacts_for_notes.json"
              curl -fsSL -H "Authorization: Bearer $GH_TOKEN_EFF" -H "Accept: application/vnd.github+json" \
                "https://api.github.com/repos/${{ github.repository }}/actions/workflows" -o "$wf_json_n" || true
              wf_id_n=$(jq -r '.workflows[] | select(.name=="Metrics E2E") | .id' "$wf_json_n" | head -n1 || true)
              if [ -n "$wf_id_n" ]; then
                # Try branch-scoped
                curl -fsSL -H "Authorization: Bearer $GH_TOKEN_EFF" -H "Accept: application/vnd.github+json" \
                  "https://api.github.com/repos/${{ github.repository }}/actions/workflows/${wf_id_n}/runs?status=success&branch=${GITHUB_REF_NAME}&per_page=1" -o "$runs_json_n" || true
                m_run_n=$(jq -r '.workflow_runs[0].id // empty' "$runs_json_n" || true)
                if [ -z "$m_run_n" ]; then
                  # Fallback to unscoped latest success
                  curl -fsSL -H "Authorization: Bearer $GH_TOKEN_EFF" -H "Accept: application/vnd.github+json" \
                    "https://api.github.com/repos/${{ github.repository }}/actions/workflows/${wf_id_n}/runs?status=success&per_page=1" -o "$runs_json_n" || true
                  m_run_n=$(jq -r '.workflow_runs[0].id // empty' "$runs_json_n" || true)
                fi
                if [ -n "$m_run_n" ]; then
                  curl -fsSL -H "Authorization: Bearer $GH_TOKEN_EFF" -H "Accept: application/vnd.github+json" \
                    "https://api.github.com/repos/${{ github.repository }}/actions/runs/${m_run_n}/artifacts" -o "$arts_json_n" || true
                  art_id_n=$(jq -r '.artifacts[] | select(.name|test("^metrics-artifacts")) | .id' "$arts_json_n" | head -n1 || true)
                  if [ -z "$art_id_n" ]; then
                    art_id_n=$(jq -r '.artifacts[0].id // empty' "$arts_json_n" || true)
                  fi
                  if [ -n "$art_id_n" ]; then
                    zip_path_n="$out_dir/_metrics_e2e_${m_run_n}_notes.zip"
                    tmp_ext_n="$out_dir/_metrics_e2e_extract_${m_run_n}_notes"
                    mkdir -p "$tmp_ext_n"
                    curl -fsSL -H "Authorization: Bearer $GH_TOKEN_EFF" -H "Accept: application/vnd.github+json" \
                      -o "$zip_path_n" \
                      "https://api.github.com/repos/${{ github.repository }}/actions/artifacts/${art_id_n}/zip" || true
                    unzip -o "$zip_path_n" -d "$tmp_ext_n" >/dev/null || true
                    found_n=$(find "$tmp_ext_n" -type f -name 'metrics_e2e_summary.txt' | head -n1 || true)
                    if [ -n "$found_n" ]; then
                      cp -f "$found_n" "$cs_alt" || true
                      echo "[notes] copied metrics_e2e_summary.txt from $found_n" >&2
                    fi
                    found_api_n=$(find "$tmp_ext_n" -type f -name 'api_metrics.prom' | head -n1 || true)
                    if [ -n "$found_api_n" ]; then
                      cp -f "$found_api_n" "$out_dir/api_metrics.prom" || true
                      echo "[notes] copied api_metrics.prom from $found_api_n" >&2
                    fi
                  fi
                fi
              fi
            fi
            # Final fallback: if api_metrics.prom missing under out_dir, try to copy from any extracted E2E dirs
            metrics_prom="$out_dir/api_metrics.prom"
            if [ ! -f "$metrics_prom" ]; then
              cand=$(find "$out_dir" -maxdepth 6 -type f -name 'api_metrics.prom' | head -n1 || true)
              if [ -n "$cand" ] && [ -f "$cand" ]; then
                cp -f "$cand" "$metrics_prom" || true
                echo "[fallback] copied api_metrics.prom from: $cand" >&2
              else
                # Try metrics_probe.prom as ultimate fallback
                cand_probe=$(find "$out_dir" -maxdepth 6 -type f -name 'metrics_probe.prom' | head -n1 || true)
                if [ -n "$cand_probe" ] && [ -f "$cand_probe" ]; then
                  cp -f "$cand_probe" "$metrics_prom" || true
                  echo "[fallback] copied metrics_probe.prom as api_metrics.prom from: $cand_probe" >&2
                fi
              fi
            fi
            # Debug: list out_dir content top-level to aid troubleshooting
            echo "-- OUT_DIR LISTING --" >&2
            (cd "$out_dir" && ls -la | sed -n '1,120p') >&2 || true
            # helper: extract first match from a file by regex, trimming (sed-only for portability)
            ext() {
              local file="$1" pat="$2"
              [ -f "$file" ] || return 1
              sed -n -E "/${pat}/p" "$file" | head -n1 | sed -E 's/^\s+//;s/\s+$//' || true
            }
            # ask success
            ask_line=$(ext "$cs_main" '^-\s*success_rate:')
            if [ -z "$ask_line" ]; then ask_line=$(ext "$cs_alt" '^-\s*success_rate:'); fi
            ask_rate=$(printf "%s" "$ask_line" | sed -E 's/^[^-]*-\s*success_rate:\s*//')
            # LLM/RAG p95
            llm_line=$(ext "$cs_main" '^-\s*LLM generate p95:')
            if [ -z "$llm_line" ]; then llm_line=$(ext "$cs_alt" '^-\s*LLM generate p95:'); fi
            llm_p95=$(printf "%s" "$llm_line" | sed -E 's/.*:\s*([0-9.]+)\s*s.*/\1 s/')
            rag_line=$(ext "$cs_main" '^-\s*RAG retrieval p95:')
            if [ -z "$rag_line" ]; then rag_line=$(ext "$cs_alt" '^-\s*RAG retrieval p95:'); fi
            rag_p95=$(printf "%s" "$rag_line" | sed -E 's/.*:\s*([0-9.]+)\s*s.*/\1 s/')
            # samples
            llm_samples_line=$(ext "$cs_main" '^-\s*LLM samples:')
            if [ -z "$llm_samples_line" ]; then llm_samples_line=$(ext "$cs_alt" '^-\s*LLM samples:'); fi
            llm_samples=$(printf "%s" "$llm_samples_line" | sed -E 's/^[^-]*-\s*LLM samples:\s*//')
            rag_samples_line=$(ext "$cs_main" '^-\s*RAG samples:')
            if [ -z "$rag_samples_line" ]; then rag_samples_line=$(ext "$cs_alt" '^-\s*RAG samples:'); fi
            rag_samples=$(printf "%s" "$rag_samples_line" | sed -E 's/^[^-]*-\s*RAG samples:\s*//')
            # extra endpoints
            upsert_block=$(ext "$cs_main" '^-\s*endpoint:\s*/embedding/upsert')
            if [ -z "$upsert_block" ]; then upsert_block=$(ext "$cs_alt" '^-\s*endpoint:\s*/embedding/upsert'); fi
            upsert_rate=""
            if [ -n "$upsert_block" ]; then
              upsert_rate=$(awk 'found{ if ($0 ~ /success_rate:/){sub(/.*success_rate: /,""); print; exit} } $0 ~ /endpoint: \/embedding\/upsert/{found=1}' "$cs_main" 2>/dev/null | head -n1)
              if [ -z "$upsert_rate" ] && [ -f "$cs_alt" ]; then
                upsert_rate=$(awk 'found{ if ($0 ~ /success_rate:/){sub(/.*success_rate: /,""); print; exit} } $0 ~ /endpoint: \/embedding\/upsert/{found=1}' "$cs_alt" 2>/dev/null | head -n1)
              fi
            fi
            pf_block=$(ext "$cs_main" '^-\s*endpoint:\s*/api/v1/rag/preflight')
            if [ -z "$pf_block" ]; then pf_block=$(ext "$cs_alt" '^-\s*endpoint:\s*/api/v1/rag/preflight'); fi
            preflight_rate=""
            if [ -n "$pf_block" ]; then
              preflight_rate=$(awk 'found{ if ($0 ~ /success_rate:/){sub(/.*success_rate: /,""); print; exit} } $0 ~ /endpoint: \/api\/v1\/rag\/preflight/{found=1}' "$cs_main" 2>/dev/null | head -n1)
              if [ -z "$preflight_rate" ] && [ -f "$cs_alt" ]; then
                preflight_rate=$(awk 'found{ if ($0 ~ /success_rate:/){sub(/.*success_rate: /,""); print; exit} } $0 ~ /endpoint: \/api\/v1\/rag\/preflight/{found=1}' "$cs_alt" 2>/dev/null | head -n1)
              fi
            fi
            # Fallbacks from Prometheus api_metrics.prom
            metrics_prom="$out_dir/api_metrics.prom"
            if [ -f "$metrics_prom" ]; then
              # helper: success rate for a given path from http_requests_total
              rate_for_path() {
                local p="$1"; local f="$2"; local total=0 ok=0
                # sum all statuses for path
                while read -r line; do
                  val=$(printf '%s' "$line" | sed -E 's/.*} *([0-9.]+).*/\1/')
                  total=$(awk -v a="$total" -v b="$val" 'BEGIN{printf("%f", a+b)}')
                done < <(grep -E "^http_requests_total\\{[^}]*path=\"$p\"[^}]*\}" "$f" || true)
                while read -r line; do
                  val=$(printf '%s' "$line" | sed -E 's/.*} *([0-9.]+).*/\1/')
                  ok=$(awk -v a="$ok" -v b="$val" 'BEGIN{printf("%f", a+b)}')
                done < <(grep -E "^http_requests_total\\{[^}]*path=\"$p\"[^}]*status=\"200\"[^}]*\}" "$f" || true)
                if [ "$total" != "0" ]; then
                  awk -v a="$ok" -v b="$total" 'BEGIN{printf("%.2f%%", (b>0? (a/b*100.0): 0))}'
                fi
              }
              # helper: p95 for histogram buckets: metric_bucket{le="x"...} (portable, no gawk asort)
              p95_for_metric() {
                local name="$1"; local f="$2"
                local total target
                total=$(grep -E "^${name}_count\\{" "$f" | awk '{s+=$NF} END{printf("%f", s+0)}') || total="0"
                if [ -z "$total" ] || [ "$total" = "0" ]; then return; fi
                target=$(awk -v t="$total" 'BEGIN{printf("%f", t*0.95)}')
                grep -E "^${name}_bucket\\{.*le=\"[^\"]+\"" "$f" | \
                  sed -E 's/.*le=\"([^\"]+)\".*} *([0-9.]+).*/\1 \2/' | \
                  awk '{a[$1]+=$2} END{for (k in a) print k, a[k]}' | \
                  awk '$1!="+Inf"{print $0}' | sort -k1,1g | \
                  awk -v target="$target" 'BEGIN{acc=0} {acc+=$2; if (acc>=target && res==""){res=$1}} END{if(res!="") print res; else print ""}'
              }
              [ -z "$ask_rate" ] && ask_rate=$(rate_for_path "/api/v1/ask" "$metrics_prom") || true
              [ -z "$upsert_rate" ] && upsert_rate=$(rate_for_path "/embedding/upsert" "$metrics_prom") || true
              [ -z "$preflight_rate" ] && preflight_rate=$(rate_for_path "/api/v1/rag/preflight" "$metrics_prom") || true
              if [ -z "$llm_p95" ]; then
                p=$(p95_for_metric "llm_generate_duration_seconds" "$metrics_prom" | head -n1)
                [ -n "$p" ] && llm_p95="$p s"
              fi
              if [ -z "$rag_p95" ]; then
                p=$(p95_for_metric "rag_retrieval_duration_seconds" "$metrics_prom" | head -n1)
                [ -n "$p" ] && rag_p95="$p s"
              fi
            fi
            # Hard fallback to ensure section is not empty
            if [ -z "${ask_rate}${preflight_rate}${upsert_rate}${llm_p95}${rag_p95}${llm_samples}${rag_samples}" ]; then
              ask_rate="N/A"; preflight_rate="N/A"; upsert_rate="N/A"; llm_p95="N/A"; rag_p95="N/A"; llm_samples="N/A"; rag_samples="N/A"
            fi
            {
              echo;
              echo "## Key Signals";
              echo;
              [ -n "$ask_rate" ] && echo "- /api/v1/ask success: $ask_rate" || true
              [ -n "$preflight_rate" ] && echo "- /api/v1/rag/preflight success: $preflight_rate" || true
              [ -n "$upsert_rate" ] && echo "- /embedding/upsert success: $upsert_rate" || true
              [ -n "$llm_p95" ] && echo "- LLM p95: $llm_p95" || true
              [ -n "$rag_p95" ] && echo "- RAG p95: $rag_p95" || true
              [ -n "$llm_samples" ] && echo "- LLM samples: $llm_samples" || true
              [ -n "$rag_samples" ] && echo "- RAG samples: $rag_samples" || true
              # debug dump (HTML comment)
              echo "<!-- KS_DEBUG ask=$ask_rate pf=$preflight_rate upsert=$upsert_rate llm_p95=$llm_p95 rag_p95=$rag_p95 llm_samples=$llm_samples rag_samples=$rag_samples src_cs=$cs_main alt_cs=$cs_alt has_prom=$( [ -f \"$metrics_prom\" ] && echo 1 || echo 0 ) -->"
            } >> "$out_dir/RELEASE_NOTES.md"

            # Console debug group for CI logs
            echo "::group::KS DEBUG"
            echo "ask_rate=$ask_rate"
            echo "preflight_rate=$preflight_rate"
            echo "upsert_rate=$upsert_rate"
            echo "llm_p95=$llm_p95 rag_p95=$rag_p95"
            echo "llm_samples=$llm_samples rag_samples=$rag_samples"
            echo "has_metrics_prom=$([ -f \"$metrics_prom\" ] && echo 1 || echo 0)"
            echo "-- RELEASE_NOTES.md (first 120 lines) --"
            sed -n '1,120p' "$out_dir/RELEASE_NOTES.md" || true
            echo "::endgroup::"
          fi
          # Append Run Links (metrics-e2e latest success, backup-restore source, this release)
          {
            echo;
            echo "## Run Links";
            echo;
            # backup-restore run
            echo "- backup-restore: https://github.com/${{ github.repository }}/actions/runs/${run_id}";
            # release run
            echo "- release: https://github.com/${{ github.repository }}/actions/runs/${GITHUB_RUN_ID}";
            # metrics-e2e latest success
            m_url=""
            wf_json="$out_dir/_workflows.json";
            runs_json="$out_dir/_metrics_e2e_runs.json";
            curl -fsSL -H "Authorization: Bearer $GH_TOKEN" -H "Accept: application/vnd.github+json" \
              "https://api.github.com/repos/${{ github.repository }}/actions/workflows" -o "$wf_json" || true
            wf_id=$(jq -r '.workflows[] | select(.name=="Metrics E2E") | .id' "$wf_json" | head -n1 || true)
            if [ -n "$wf_id" ]; then
              curl -fsSL -H "Authorization: Bearer $GH_TOKEN" -H "Accept: application/vnd.github+json" \
                "https://api.github.com/repos/${{ github.repository }}/actions/workflows/${wf_id}/runs?status=success&per_page=1" -o "$runs_json" || true
              m_run=$(jq -r '.workflow_runs[0].id // empty' "$runs_json" || true)
              if [ -n "$m_run" ]; then
                m_url="https://github.com/${{ github.repository }}/actions/runs/${m_run}"
                echo "- metrics-e2e: $m_url";
              fi
            fi
          } >> "$out_dir/RELEASE_NOTES.md"
          # Decide if release notes changed compared to existing release body (after all sections appended)
          notes_hash=$(shasum -a 256 "$out_dir/RELEASE_NOTES.md" | awk '{print $1}')
          changed="true"
          tmp_rel="$out_dir/_existing_release.json"
          if curl -fsSL -H "Authorization: Bearer $GH_TOKEN" -H "Accept: application/vnd.github+json" \
              "https://api.github.com/repos/${{ github.repository }}/releases/tags/${TAG_NAME}" -o "$tmp_rel"; then
            body=$(jq -r '.body // ""' "$tmp_rel")
            old_hash=$(printf "%s" "$body" | shasum -a 256 | awk '{print $1}')
            if [ "$old_hash" = "$notes_hash" ]; then
              changed="false"
            fi
          fi
          # Always emit outputs so downstream steps can guard properly
          echo "notes_path=$out_dir/RELEASE_NOTES.md" >> "$GITHUB_OUTPUT" || true
          echo "changed=$changed" >> "$GITHUB_OUTPUT" || true
          # Do not fail this step due to optional content issues
          exit 0

      - name: Create or Update GitHub Release (only if changed/new or forced)
        if: steps.notes.outputs.changed == 'true' || inputs.force == true
        uses: softprops/action-gh-release@v2
        with:
          tag_name: ${{ inputs.tag || format('backup-restore-run-{0}', steps.resolve_run.outputs.run_id) }}
          name: ${{ inputs.name || format('Backup Restore Artifacts (run {0})', steps.resolve_run.outputs.run_id) }}
          body_path: ${{ steps.notes.outputs.notes_path }}
          allowUpdates: true
          files: |
            ${{ steps.list_artifacts.outputs.artifact_name && format('release_artifacts/{0}/{1}.zip', steps.resolve_run.outputs.run_id, steps.list_artifacts.outputs.artifact_name) }}
            ${{ steps.list_artifacts.outputs.artifact_name && format('release_artifacts/{0}/{1}.zip.sha256', steps.resolve_run.outputs.run_id, steps.list_artifacts.outputs.artifact_name) }}
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Notify release success (Slack/Lark optional)
        if: always()
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
          LARK_WEBHOOK_URL: ${{ secrets.LARK_WEBHOOK_URL }}
          # Default thresholds (main)
          WARN_API_SUCCESS_MIN: "98"   # percent threshold for warning
          WARN_LLM_P95_MAX: "1.8"      # seconds
          WARN_RAG_P95_MAX: "0.9"      # seconds
          # Branch name resolved from trigger (workflow_run or workflow_dispatch)
          BRANCH_NAME: ${{ github.event.workflow_run.head_branch || github.ref_name }}
        run: |
          set -euo pipefail
          # Adjust thresholds by branch patterns
          case "${BRANCH_NAME:-}" in
            release/*)
              WARN_API_SUCCESS_MIN="99"; WARN_LLM_P95_MAX="1.5"; WARN_RAG_P95_MAX="0.8" ;;
            main|master)
              WARN_API_SUCCESS_MIN="98"; WARN_LLM_P95_MAX="1.8"; WARN_RAG_P95_MAX="0.9" ;;
            dev|develop|staging)
              WARN_API_SUCCESS_MIN="95"; WARN_LLM_P95_MAX="2.2"; WARN_RAG_P95_MAX="1.2" ;;
            feature/*|feat/*)
              WARN_API_SUCCESS_MIN="95"; WARN_LLM_P95_MAX="2.5"; WARN_RAG_P95_MAX="1.5" ;;
            *)
              WARN_API_SUCCESS_MIN="98"; WARN_LLM_P95_MAX="1.8"; WARN_RAG_P95_MAX="0.9" ;;
          esac
          run_id="${{ steps.resolve_run.outputs.run_id }}"
          tag_name="${{ inputs.tag || format('backup-restore-run-{0}', steps.resolve_run.outputs.run_id) }}"
          release_url="https://github.com/${{ github.repository }}/releases/tag/${tag_name}"
          notes_path='${{ steps.notes.outputs.notes_path }}'
          if [ -z "$notes_path" ] || [ ! -s "$notes_path" ]; then
            echo "notes_path is empty or missing; skip notifications."
            exit 0
          fi
          # Extract a few metrics lines to include in notification
          run_metrics=$(sed -n '/^## Run Metrics$/,/^## /p' "$notes_path" | sed '1d;$d' | head -n 10 || true)
          ci_summary=$(sed -n '/^## CI Summary$/,/^## /p' "$notes_path" | sed '1d;$d' | head -n 15 || true)
          key_signals=$(sed -n '/^## Key Signals$/,/^## /p' "$notes_path" | sed '1d;$d' | head -n 10 || true)
          # Evaluate warning flags from CI Summary
          warn_msgs=()
          # API success
          if rate_line=$(printf "%s" "$ci_summary" | grep -E '^- success_rate:' | head -n1); then
            rate_num=$(printf "%s" "$rate_line" | sed -E 's/.*: ([0-9.]+)%.*/\1/')
            min_ok=${WARN_API_SUCCESS_MIN}
            awk -v r="$rate_num" -v m="$min_ok" 'BEGIN{if (r<m) exit 0; else exit 1}' || warn_msgs+=("API success < ${min_ok}%")
          fi
          # /embedding/upsert success (from extra APIs section)
          if up_line=$(printf "%s" "$ci_summary" | awk '
            $0 ~ /^- endpoint: \/embedding\/upsert$/ {flag=1; next}
            flag && $0 ~ /success_rate:/ { print; flag=0 }
          ' | head -n1); then
            up_rate=$(printf "%s" "$up_line" | sed -E 's/.*success_rate: ([0-9.]+)%.*/\1/')
            min_ok=${WARN_API_SUCCESS_MIN}
            awk -v r="$up_rate" -v m="$min_ok" 'BEGIN{if (r<m) exit 0; else exit 1}' || warn_msgs+=("/embedding/upsert success < ${min_ok}%")
          fi
          # /api/v1/rag/preflight success
          if pf_line=$(printf "%s" "$ci_summary" | awk '
            $0 ~ /^- endpoint: \/api\/v1\/rag\/preflight$/ {flag=1; next}
            flag && $0 ~ /success_rate:/ { print; flag=0 }
          ' | head -n1); then
            pf_rate=$(printf "%s" "$pf_line" | sed -E 's/.*success_rate: ([0-9.]+)%.*/\1/')
            min_ok=${WARN_API_SUCCESS_MIN}
            awk -v r="$pf_rate" -v m="$min_ok" 'BEGIN{if (r<m) exit 0; else exit 1}' || warn_msgs+=("/api/v1/rag/preflight success < ${min_ok}%")
          fi
          # Latency p95
          if llm95=$(printf "%s" "$ci_summary" | grep -E 'LLM generate p95' | sed -E 's/.*: ([0-9.]+) s.*/\1/' | head -n1); then
            awk -v v="$llm95" -v m="$WARN_LLM_P95_MAX" 'BEGIN{if (v>m) exit 0; else exit 1}' || warn_msgs+=("LLM p95 > ${WARN_LLM_P95_MAX}s")
          fi
          if rag95=$(printf "%s" "$ci_summary" | grep -E 'RAG retrieval p95' | sed -E 's/.*: ([0-9.]+) s.*/\1/' | head -n1); then
            awk -v v="$rag95" -v m="$WARN_RAG_P95_MAX" 'BEGIN{if (v>m) exit 0; else exit 1}' || warn_msgs+=("RAG p95 > ${WARN_RAG_P95_MAX}s")
          fi
          warn_head=""
          if [ ${#warn_msgs[@]} -gt 0 ]; then
            warn_head="⚠️ Warnings: ${warn_msgs[*]}\n\n"
          fi

          # Only send notifications when warnings are present
          should_notify="false"
          if [ ${#warn_msgs[@]} -gt 0 ]; then
            should_notify="true"
          fi
          if [ "$should_notify" != "true" ]; then
            echo "No warnings; skip notifications."
            exit 0
          fi

          if [ -n "${SLACK_WEBHOOK_URL:-}" ]; then
            payload=$(jq -n --arg text "[AI Support] Release published: ${tag_name}\n${release_url}\n\n${warn_head}Key Signals:\n${key_signals}\n\nRun Metrics:\n${run_metrics}\n\nCI Summary:\n${ci_summary}" '{text:$text}')
            curl -fsSL -H 'Content-Type: application/json' -d "$payload" "$SLACK_WEBHOOK_URL" || true
          fi
          if [ -n "${LARK_WEBHOOK_URL:-}" ]; then
            payload=$(jq -n --arg msg "[AI Support] Release published: ${tag_name}\n${release_url}\n\n${warn_head}Key Signals:\n${key_signals}\n\nRun Metrics:\n${run_metrics}\n\nCI Summary:\n${ci_summary}" '{msg_type:"text", content:{text:$msg}}')
            curl -fsSL -H 'Content-Type: application/json' -d "$payload" "$LARK_WEBHOOK_URL" || true
          fi

      - name: Print release info
        if: always()
        run: |
          echo "Released artifacts for run: ${{ steps.resolve_run.outputs.run_id }}"
          echo "Tag: ${{ inputs.tag || format('backup-restore-run-{0}', steps.resolve_run.outputs.run_id) }}"
