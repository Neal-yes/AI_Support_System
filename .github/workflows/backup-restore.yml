name: Backup & Restore Qdrant

on:
  workflow_dispatch:
  schedule:
    # Daily at 03:00 Asia/Shanghai (19:00 UTC)
    - cron: '0 19 * * *'

permissions:
  contents: read
  actions: read

jobs:
  backup-restore:
    name: backup-restore
    runs-on: ubuntu-latest
    timeout-minutes: 45

    env:
      API_BASE: http://localhost:8000
      QDRANT_HTTP: http://localhost:6333
      OUT_DIR: artifacts/metrics
      RAG_COLLECTION: default_collection
      BATCH_SIZE: "64"
      MAX_DOCS: "500"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Ensure artifacts directory exists
        run: |
          mkdir -p ${OUT_DIR}

      - name: Prepare .env for Compose
        run: |
          set -euo pipefail
          cat > .env <<'EOF'
          POSTGRES_USER=postgres
          POSTGRES_PASSWORD=postgres
          POSTGRES_DB=ai_support
          API_PORT=8000
          QDRANT_PORT=6333
          REDIS_PORT=6379
          OLLAMA_PORT=11434
          EOF

      - name: Set up Docker Compose
        uses: KengoTODA/actions-setup-docker-compose@v1
        with:
          version: v2.27.1

      - name: Launch minimal stack (api + qdrant + ollama + db)
        run: |
          set -euxo pipefail
          docker compose pull || true
          docker compose up -d db qdrant ollama api
          docker compose ps

      - name: Wait /-/ready
        run: |
          set -euo pipefail
          url="${API_BASE}/-/ready"
          echo "Waiting for $url ..."
          for i in $(seq 1 150); do
            code=$(curl -sS -o /tmp/ready.json -w "%{http_code}" "$url" || true)
            if [ "$code" = "200" ] && jq -e '.status == "ok" or .status == "degraded"' /tmp/ready.json >/dev/null 2>&1; then
              echo "API ready ($code)"; break
            fi
            sleep 2
          done

      - name: Seed demo embeddings
        run: |
          set -euxo pipefail
          export API_BASE="${API_BASE}"
          export OUT_DIR="${OUT_DIR}"
          export MAX_DOCS="${MAX_DOCS}"
          # Ensure seeding uses the same collection as backup/restore later
          export RAG_COLLECTION="${RAG_COLLECTION}"
          bash scripts/ci/prepare_demo_embeddings.sh

      - name: Capture seed stats
        id: seed
        run: |
          set -euo pipefail
          summary="${OUT_DIR}/embedding_upsert.json"
          total=$(jq -r '.total // 0' "$summary")
          src=$(jq -r '.src // ""' "$summary")
          coll=$(jq -r '.collection // empty' "$summary")
          if [ -z "$coll" ]; then coll="${RAG_COLLECTION}"; fi
          echo "total=$total" >> $GITHUB_OUTPUT
          echo "src=$src" >> $GITHUB_OUTPUT
          echo "coll=$coll" >> $GITHUB_OUTPUT

      - name: Detect actual populated collection
        id: detect
        run: |
          set -euo pipefail
          intended="${{ steps.seed.outputs.coll }}"
          echo "Intended collection from seed: $intended"
          # If intended exists, prefer it
          if curl -fsS "${QDRANT_HTTP}/collections" | jq -e --arg c "$intended" '.result.collections | any(.name== $c)' >/dev/null 2>&1; then
            echo "coll=$intended" >> $GITHUB_OUTPUT
            exit 0
          fi
          echo "Intended collection '$intended' not found in Qdrant. Discovering by highest points count..." >&2
          # List collections and pick the one with max points count
          cols=$(curl -fsS "${QDRANT_HTTP}/collections" | jq -r '.result.collections[].name')
          best=""; best_cnt=0
          for c in $cols; do
            cnt=$(curl -fsS -X POST -H 'Content-Type: application/json' \
              -d '{"exact":true}' \
              "${QDRANT_HTTP}/collections/${c}/points/count" | jq -r '.result.count // 0' 2>/dev/null || echo 0)
            echo "Collection ${c}: count=${cnt}"
            if [ "${cnt}" -gt "${best_cnt}" ]; then best_cnt=${cnt}; best=${c}; fi
          done
          if [ -z "$best" ]; then
            echo "No collections found in Qdrant after seeding" >&2
            exit 1
          fi
          echo "Using detected collection: $best (count=$best_cnt)"
          echo "coll=$best" >> $GITHUB_OUTPUT

      - name: Backup start timestamp
        id: t0
        run: echo "ts=$(date +%s)" >> $GITHUB_OUTPUT

      - name: Backup Qdrant collection
        run: |
          set -euxo pipefail
          export QDRANT_HTTP="${QDRANT_HTTP}"
          export RAG_COLLECTION="${{ steps.detect.outputs.coll }}"
          export OUT_DIR="${OUT_DIR}"
          bash scripts/ci/backup_qdrant_collection.sh

      - name: Backup stats
        id: bk
        run: |
          set -euo pipefail
          dump="${OUT_DIR}/qdrant_${{ steps.seed.outputs.coll }}_dump.json"
          if [ ! -f "$dump" ]; then echo "missing dump: $dump" >&2; exit 1; fi
          bk_total=$(jq 'length' "$dump")
          echo "bk_total=$bk_total" >> $GITHUB_OUTPUT

      - name: Drop collection
        run: |
          set -euxo pipefail
          curl -fsS -X DELETE "${QDRANT_HTTP}/collections/${{ steps.detect.outputs.coll }}" || true
          # verify deleted by listing collections
          curl -fsS "${QDRANT_HTTP}/collections" | jq -e '.result.collections | map(.name) | index("${{ steps.detect.outputs.coll }}") | not' >/dev/null

      - name: Restore start timestamp
        id: t1
        run: echo "ts=$(date +%s)" >> $GITHUB_OUTPUT

      - name: Warmup embedding model
        run: |
          set -euo pipefail
          # 注意：预热写入临时集合，避免污染目标集合计数
          body='{"texts":["warmup"],"payloads":[{}],"collection":"'"${{ steps.detect.outputs.coll }}__warmup"'"}'
          # best-effort warmup, ignore non-200 to not fail pipeline here
          curl -sS -o /tmp/warmup.json -w "%{http_code}" -H 'Content-Type: application/json' -d "$body" "${API_BASE}/embedding/upsert" || true

      - name: Restore from backup
        run: |
          set -euxo pipefail
          export API_BASE="${API_BASE}"
          export RAG_COLLECTION="${{ steps.detect.outputs.coll }}"
          export OUT_DIR="${OUT_DIR}"
          export SRC_DUMP="${OUT_DIR}/qdrant_${{ steps.detect.outputs.coll }}_dump.json"
          export BATCH_SIZE="${BATCH_SIZE}"
          # retry up to 3 times in case of transient model init/network issues
          n=0
          until [ "$n" -ge 3 ]; do
            if bash scripts/ci/restore_qdrant_collection.sh; then
              break
            fi
            n=$((n+1))
            echo "[RESTORE] retry $n/3 after failure..." >&2
            sleep 5
          done
          if [ "$n" -ge 3 ]; then
            echo "[RESTORE] failed after retries" >&2
            exit 1
          fi

      - name: Verify restored count via scroll
        id: rv
        run: |
          set -euo pipefail
          # count points by paging
          coll="${{ steps.detect.outputs.coll }}"
          next="null"
          total=0
          while :; do
            body=$(jq -n --argjson lim 512 --argjson next "$next" '{limit:$lim, with_payload:false, with_vectors:false} + (if $next==null then {} else {offset:$next} end)')
            resp=$(curl -sS -X POST -H 'Content-Type: application/json' -d "$body" "${QDRANT_HTTP}/collections/${coll}/points/scroll")
            status=$(echo "$resp" | jq -r '.status')
            if [ "$status" != "ok" ]; then echo "scroll failed: $resp" >&2; exit 1; fi
            cnt=$(echo "$resp" | jq '.result.points | length')
            total=$(( total + cnt ))
            next=$(echo "$resp" | jq -c '.result.next_page_offset')
            has_next=$(echo "$resp" | jq '.result.next_page_offset != null')
            if [ "$has_next" != "true" ]; then break; fi
          done
          echo "restored_total=$total" >> $GITHUB_OUTPUT

      - name: Compute durations and report
        run: |
          set -euo pipefail
          t0=${{ steps.t0.outputs.ts }}
          t1=${{ steps.t1.outputs.ts }}
          t2=$(date +%s)
          rto=$(( t2 - t1 ))
          bkt=$(( t1 - t0 ))
          echo "Backup duration (s): $bkt"
          echo "Restore duration (s): $rto"
          {
            echo "# Backup & Restore Summary"
            echo
            echo "- collection: ${{ steps.seed.outputs.coll }}"
            echo "- seed_total: ${{ steps.seed.outputs.total }}"
            echo "- backup_total: ${{ steps.bk.outputs.bk_total }}"
            echo "- restored_total: ${{ steps.rv.outputs.restored_total }}"
            echo "- backup_duration_seconds: $bkt"
            echo "- restore_duration_seconds (RTO): $rto"
            echo "- src: ${{ steps.seed.outputs.src }}"
          } >> "$GITHUB_STEP_SUMMARY"
          # hard assert equality
          test "${{ steps.bk.outputs.bk_total }}" = "${{ steps.rv.outputs.restored_total }}"

      - name: Emit consolidated CI summary file
        env:
          GH_TOKEN: ${{ github.token }}
        run: |
          set -euo pipefail
          mkdir -p "${OUT_DIR}"
          ci_file="${OUT_DIR}/ci_summary.txt"
          e2e_file="${OUT_DIR}/metrics_e2e_summary.txt"
          # Try to fetch latest successful metrics-e2e artifact and extract metrics_e2e_summary.txt
          echo "Fetching latest successful Metrics E2E run..."
          wf_json="${OUT_DIR}/metrics_e2e_workflows.json"
          runs_json="${OUT_DIR}/metrics_e2e_runs.json"
          curl -fsSL -H "Authorization: Bearer $GH_TOKEN" -H "Accept: application/vnd.github+json" \
            "https://api.github.com/repos/${{ github.repository }}/actions/workflows" -o "$wf_json"
          wf_id=$(jq -r '.workflows[] | select(.name=="Metrics E2E") | .id' "$wf_json" | head -n1 || true)
          if [ -n "$wf_id" ]; then
            curl -fsSL -H "Authorization: Bearer $GH_TOKEN" -H "Accept: application/vnd.github+json" \
              "https://api.github.com/repos/${{ github.repository }}/actions/workflows/${wf_id}/runs?status=success&per_page=1" -o "$runs_json"
            m_run=$(jq -r '.workflow_runs[0].id // empty' "$runs_json" || true)
            if [ -n "$m_run" ]; then
              arts_json="${OUT_DIR}/metrics_e2e_artifacts_${m_run}.json"
              curl -fsSL -H "Authorization: Bearer $GH_TOKEN" -H "Accept: application/vnd.github+json" \
                "https://api.github.com/repos/${{ github.repository }}/actions/runs/${m_run}/artifacts" -o "$arts_json"
              art_id=$(jq -r '.artifacts[0].id // empty' "$arts_json" || true)
              if [ -n "$art_id" ]; then
                zip_path="${OUT_DIR}/metrics_e2e_${m_run}.zip"
                curl -fsSL -H "Authorization: Bearer $GH_TOKEN" -H "Accept: application/vnd.github+json" \
                  -o "$zip_path" \
                  "https://api.github.com/repos/${{ github.repository }}/actions/artifacts/${art_id}/zip"
                tmp_ext="${OUT_DIR}/_metrics_e2e_extract_${m_run}"
                mkdir -p "$tmp_ext"
                unzip -o "$zip_path" -d "$tmp_ext" >/dev/null || true
                found=$(find "$tmp_ext" -type f -name 'metrics_e2e_summary.txt' | head -n1 || true)
                if [ -n "$found" ]; then
                  cp -f "$found" "$e2e_file"
                  echo "Imported metrics_e2e_summary.txt from metrics-e2e run ${m_run}"
                fi
              fi
            fi
          fi
          {
            echo "# Consolidated CI Summary";
            echo;
            echo "- collection: ${{ steps.seed.outputs.coll }}";
            echo "- seed_total: ${{ steps.seed.outputs.total }}";
            echo "- backup_total: ${{ steps.bk.outputs.bk_total }}";
            echo "- restored_total: ${{ steps.rv.outputs.restored_total }}";
            echo "- src: ${{ steps.seed.outputs.src }}";
            echo "- backup_duration_seconds: $(($(date +%s) - ${{ steps.t0.outputs.ts }}))";
            echo "- restore_duration_seconds (RTO): $(($(date +%s) - ${{ steps.t1.outputs.ts }}))";
          } > "$ci_file"
          # Ensure metrics_e2e_summary exists; external workflows may write into it beforehand.
          if [ ! -s "$e2e_file" ]; then
            {
              echo "# Metrics E2E Summary";
              echo;
              echo "(not available in this run)";
            } > "$e2e_file"
          fi
          {
            echo;
            echo "# Metrics E2E Summary";
            echo;
            sed -n '1,200p' "$e2e_file" || true;
          } >> "$ci_file"

      - name: Validate artifacts
        run: |
          set -euo pipefail
          python3 scripts/ci/validate_backup_artifacts.py \
            --emb artifacts/metrics/embedding_upsert.json \
            --dump artifacts/metrics/qdrant_${{ steps.seed.outputs.coll }}_dump.json \
            --expect-total ${{ steps.seed.outputs.total }} \
            --expect-src ${{ steps.seed.outputs.src }} \
            --expect-collection ${{ steps.seed.outputs.coll }}

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: backup-restore-${{ github.run_id }}-1
          path: |
            artifacts/metrics/embedding_upsert.json
            artifacts/metrics/qdrant_${{ steps.seed.outputs.coll }}_dump.json
            artifacts/metrics/ci_summary.txt
            artifacts/metrics/metrics_e2e_summary.txt
          retention-days: 14

      - name: Teardown
        if: always()
        run: |
          set -euxo pipefail
          docker compose down -v || true
